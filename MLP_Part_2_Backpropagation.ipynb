{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqeB8IY24IPG",
        "outputId": "b2cc80c0-c95f-4915-efce-c43b672e87ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ">>> ReLU Forward [Layer 0]\n",
            "Input to ReLU:\n",
            "tensor([[5., 4., 3., 7., 9.],\n",
            "        [3., 4., 5., 9., 7.]], grad_fn=<BackwardHookFunctionBackward>)\n",
            "Output from ReLU:\n",
            "tensor([[5., 4., 3., 7., 9.],\n",
            "        [3., 4., 5., 9., 7.]], grad_fn=<ReluBackward0>)\n",
            "\n",
            ">>> ReLU Forward [Layer 1]\n",
            "Input to ReLU:\n",
            "tensor([[6., 1.],\n",
            "        [2., 7.]], grad_fn=<BackwardHookFunctionBackward>)\n",
            "Output from ReLU:\n",
            "tensor([[6., 1.],\n",
            "        [2., 7.]], grad_fn=<ReluBackward0>)\n",
            "\n",
            ">>> ReLU Forward [Layer 2]\n",
            "Input to ReLU:\n",
            "tensor([[ 7.,  5.,  8.],\n",
            "        [ 9., -5., 16.]], grad_fn=<BackwardHookFunctionBackward>)\n",
            "Output from ReLU:\n",
            "tensor([[ 7.,  5.,  8.],\n",
            "        [ 9.,  0., 16.]], grad_fn=<ReluBackward0>)\n",
            "\n",
            ">>> ReLU Forward [Layer 3]\n",
            "Input to ReLU:\n",
            "tensor([[ 2.,  3.],\n",
            "        [ 9., 16.]], grad_fn=<BackwardHookFunctionBackward>)\n",
            "Output from ReLU:\n",
            "tensor([[ 2.,  3.],\n",
            "        [ 9., 16.]], grad_fn=<ReluBackward0>)\n",
            "\n",
            ">>> ReLU Forward [Layer 4]\n",
            "Input to ReLU:\n",
            "tensor([[ 3.,  2.],\n",
            "        [16.,  9.]], grad_fn=<BackwardHookFunctionBackward>)\n",
            "Output from ReLU:\n",
            "tensor([[ 3.,  2.],\n",
            "        [16.,  9.]], grad_fn=<ReluBackward0>)\n",
            "\n",
            ">>> ReLU Forward [Layer 5]\n",
            "Input to ReLU:\n",
            "tensor([[ 1.,  5.],\n",
            "        [ 7., 25.]], grad_fn=<BackwardHookFunctionBackward>)\n",
            "Output from ReLU:\n",
            "tensor([[ 1.,  5.],\n",
            "        [ 7., 25.]], grad_fn=<ReluBackward0>)\n",
            "Loss: 212.5\n",
            "grad_output from loss: tensor(1.)\n",
            "grad_input from loss: tensor([[ -5.],\n",
            "        [-20.]])\n",
            "\n",
            "<<< Linear Backward [Layer 6]\n",
            "grad_input:\n",
            "(tensor([[ -5.,   5.],\n",
            "        [-20.,  20.]]),)\n",
            "grad_output:\n",
            "(tensor([[ -5.],\n",
            "        [-20.]]),)\n",
            "\n",
            "<<< ReLU Backward [Layer 5]\n",
            "Gradient BEFORE ReLU (grad_output):\n",
            "tensor([[ -5.,   5.],\n",
            "        [-20.,  20.]])\n",
            "Gradient AFTER ReLU (grad_input):\n",
            "tensor([[ -5.,   5.],\n",
            "        [-20.,  20.]])\n",
            "\n",
            "<<< Linear Backward [Layer 5]\n",
            "grad_input:\n",
            "(tensor([[ 0., 10.],\n",
            "        [ 0., 40.]]),)\n",
            "grad_output:\n",
            "(tensor([[ -5.,   5.],\n",
            "        [-20.,  20.]]),)\n",
            "\n",
            "<<< ReLU Backward [Layer 4]\n",
            "Gradient BEFORE ReLU (grad_output):\n",
            "tensor([[ 0., 10.],\n",
            "        [ 0., 40.]])\n",
            "Gradient AFTER ReLU (grad_input):\n",
            "tensor([[ 0., 10.],\n",
            "        [ 0., 40.]])\n",
            "\n",
            "<<< Linear Backward [Layer 4]\n",
            "grad_input:\n",
            "(tensor([[10.,  0.],\n",
            "        [40.,  0.]]),)\n",
            "grad_output:\n",
            "(tensor([[ 0., 10.],\n",
            "        [ 0., 40.]]),)\n",
            "\n",
            "<<< ReLU Backward [Layer 3]\n",
            "Gradient BEFORE ReLU (grad_output):\n",
            "tensor([[10.,  0.],\n",
            "        [40.,  0.]])\n",
            "Gradient AFTER ReLU (grad_input):\n",
            "tensor([[10.,  0.],\n",
            "        [40.,  0.]])\n",
            "\n",
            "<<< Linear Backward [Layer 3]\n",
            "grad_input:\n",
            "(tensor([[ 10., -10.,   0.],\n",
            "        [ 40., -40.,   0.]]),)\n",
            "grad_output:\n",
            "(tensor([[10.,  0.],\n",
            "        [40.,  0.]]),)\n",
            "\n",
            "<<< ReLU Backward [Layer 2]\n",
            "Gradient BEFORE ReLU (grad_output):\n",
            "tensor([[ 10., -10.,   0.],\n",
            "        [ 40., -40.,   0.]])\n",
            "Gradient AFTER ReLU (grad_input):\n",
            "tensor([[ 10., -10.,   0.],\n",
            "        [ 40.,   0.,   0.]])\n",
            "\n",
            "<<< Linear Backward [Layer 2]\n",
            "grad_input:\n",
            "(tensor([[ 0., 20.],\n",
            "        [40., 40.]]),)\n",
            "grad_output:\n",
            "(tensor([[ 10., -10.,   0.],\n",
            "        [ 40.,   0.,   0.]]),)\n",
            "\n",
            "<<< ReLU Backward [Layer 1]\n",
            "Gradient BEFORE ReLU (grad_output):\n",
            "tensor([[ 0., 20.],\n",
            "        [40., 40.]])\n",
            "Gradient AFTER ReLU (grad_input):\n",
            "tensor([[ 0., 20.],\n",
            "        [40., 40.]])\n",
            "\n",
            "<<< Linear Backward [Layer 1]\n",
            "grad_input:\n",
            "(tensor([[  0.,   0.,  20.,  20., -20.],\n",
            "        [ 40.,  40.,   0.,  40., -40.]]),)\n",
            "grad_output:\n",
            "(tensor([[ 0., 20.],\n",
            "        [40., 40.]]),)\n",
            "\n",
            "<<< ReLU Backward [Layer 0]\n",
            "Gradient BEFORE ReLU (grad_output):\n",
            "tensor([[  0.,   0.,  20.,  20., -20.],\n",
            "        [ 40.,  40.,   0.,  40., -40.]])\n",
            "Gradient AFTER ReLU (grad_input):\n",
            "tensor([[  0.,   0.,  20.,  20., -20.],\n",
            "        [ 40.,  40.,   0.,  40., -40.]])\n",
            "\n",
            "<<< Linear Backward [Layer 0]\n",
            "grad_input:\n",
            "(tensor([[ 40.,   0., -20.],\n",
            "        [ 40.,  40.,   0.]]),)\n",
            "grad_output:\n",
            "(tensor([[  0.,   0.,  20.,  20., -20.],\n",
            "        [ 40.,  40.,   0.,  40., -40.]]),)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the custom network with named ReLU layers\n",
        "class MyNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyNetwork, self).__init__()\n",
        "        self.h1 = nn.Linear(3, 5)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.h2 = nn.Linear(5, 2)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.h3 = nn.Linear(2, 3)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.h4 = nn.Linear(3, 2)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.h5 = nn.Linear(2, 2)\n",
        "        self.relu5 = nn.ReLU()\n",
        "        self.h6 = nn.Linear(2, 2)\n",
        "        self.relu6 = nn.ReLU()\n",
        "        self.h7 = nn.Linear(2, 1)\n",
        "\n",
        "        self.h1.weight.data = torch.tensor([[0, 0, 1],[0, 1, 0],[1, 0, 0],[1, 1, 0],[0, 1, 1]], dtype=torch.float32)\n",
        "        self.h2.weight.data = torch.tensor([[1, 1, -1, 0, 0],[0, 0, 1, 1, -1]], dtype=torch.float32)\n",
        "        self.h3.weight.data = torch.tensor([[1, 1],[1, -1],[1, 2]], dtype=torch.float32)\n",
        "        self.h4.weight.data = torch.tensor([[1, -1, 0],[0, -1, 1]], dtype=torch.float32)\n",
        "        self.h5.weight.data = torch.tensor([[0, 1],[1, 0]], dtype=torch.float32)\n",
        "        self.h6.weight.data = torch.tensor([[1, -1],[1, 1]], dtype=torch.float32)\n",
        "        self.h7.weight.data = torch.tensor([[1, -1]], dtype=torch.float32)\n",
        "\n",
        "        B = torch.tensor([0, 0, 0, 0, 0], dtype=torch.float32)\n",
        "        self.h1.bias.data = B\n",
        "        self.h2.bias.data = B[:2]\n",
        "        self.h3.bias.data = B[:3]\n",
        "        self.h4.bias.data = B[:2]\n",
        "        self.h5.bias.data = B[:2]\n",
        "        self.h6.bias.data = B[:2]\n",
        "        self.h7.bias.data = torch.tensor([0.], dtype=torch.float32)\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = self.relu1(self.h1(input))\n",
        "        out = self.relu2(self.h2(out))\n",
        "        out = self.relu3(self.h3(out))\n",
        "        out = self.relu4(self.h4(out))\n",
        "        out = self.relu5(self.h5(out))\n",
        "        out = self.relu6(self.h6(out))\n",
        "        out = self.h7(out)\n",
        "        return out\n",
        "\n",
        "# Custom MSE loss using autograd\n",
        "class CustomMSELossFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, target):\n",
        "        ctx.save_for_backward(input, target)\n",
        "        return ((input - target) ** 2).mean()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        input, target = ctx.saved_tensors\n",
        "        grad_input = 2.0 * (input - target) / input.size(0) * grad_output\n",
        "        print(\"grad_output from loss:\", grad_output)\n",
        "        print(\"grad_input from loss:\", grad_input )\n",
        "        return grad_input, None\n",
        "\n",
        "# Register backward hooks\n",
        "def register_hooks(model):\n",
        "    hooks = []\n",
        "\n",
        "    relu_count = 0\n",
        "    linear_count = 0\n",
        "\n",
        "    for layer in model.modules():\n",
        "        if isinstance(layer, nn.ReLU):\n",
        "            layer_id = relu_count\n",
        "            relu_count += 1\n",
        "\n",
        "            def make_relu_forward_hook(index):\n",
        "                def relu_forward_hook(module, input, output):\n",
        "                    print(f\"\\n>>> ReLU Forward [Layer {index}]\")\n",
        "                    print(f\"Input to ReLU:\\n{input[0]}\")\n",
        "                    print(f\"Output from ReLU:\\n{output}\")\n",
        "                return relu_forward_hook\n",
        "\n",
        "            def make_relu_backward_hook(index):\n",
        "                def relu_backward_hook(module, grad_input, grad_output):\n",
        "                    print(f\"\\n<<< ReLU Backward [Layer {index}]\")\n",
        "                    print(f\"Gradient BEFORE ReLU (grad_output):\\n{grad_output[0]}\")\n",
        "                    print(f\"Gradient AFTER ReLU (grad_input):\\n{grad_input[0]}\")\n",
        "                return relu_backward_hook\n",
        "\n",
        "            hooks.append(layer.register_forward_hook(make_relu_forward_hook(layer_id)))\n",
        "            hooks.append(layer.register_full_backward_hook(make_relu_backward_hook(layer_id)))\n",
        "\n",
        "        elif isinstance(layer, nn.Linear):\n",
        "            layer_id = linear_count\n",
        "            linear_count += 1\n",
        "\n",
        "            def make_linear_backward_hook(index):\n",
        "                def linear_backward_hook(module, grad_input, grad_output):\n",
        "                    print(f\"\\n<<< Linear Backward [Layer {index}]\")\n",
        "                    print(f\"grad_input:\\n{grad_input}\")\n",
        "                    print(f\"grad_output:\\n{grad_output}\")\n",
        "                return linear_backward_hook\n",
        "\n",
        "            hooks.append(layer.register_full_backward_hook(make_linear_backward_hook(layer_id)))\n",
        "\n",
        "    return hooks\n",
        "\n",
        "# Input and target\n",
        "x = torch.tensor([[3, 4, 5], [5, 4, 3]], dtype=torch.float32, requires_grad=True)\n",
        "target = torch.tensor([[1.], [2.]])\n",
        "\n",
        "# Initialize model\n",
        "model = MyNetwork()\n",
        "model.train()\n",
        "\n",
        "# Register hooks\n",
        "hooks = register_hooks(model)\n",
        "\n",
        "# Forward and backward\n",
        "output = model(x)\n",
        "loss = CustomMSELossFunction.apply(output, target)\n",
        "print(\"Loss:\", loss.item())\n",
        "loss.backward()\n",
        "\n",
        "# Clean up\n",
        "for h in hooks:\n",
        "    h.remove()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Epoch 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpDOi_35fDhd",
        "outputId": "83c9d6ad-8ce4-4e75-bd2b-c16358935dd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========== EPOCH 1 ==========\n",
            "Loss: 212.5\n",
            "**********\n",
            "2 2\n",
            "grad_output from loss: tensor(1.)\n",
            "grad_input from loss: tensor([[ -5.],\n",
            "        [-20.]])\n",
            "\n",
            "========== EPOCH 2 ==========\n",
            "\n",
            ">>> Forward Output from h1 [Layer 0]\n",
            "tensor([[-194.5000, -196.5000,  -99.0000, -283.0000,  299.0000],\n",
            "        [-212.5000, -212.5000,  -89.0000, -289.0000,  305.0000]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            ">>> Weights of h1 [Layer 0] after forward:\n",
            "tensor([[-20., -16., -11.],\n",
            "        [-20., -15., -12.],\n",
            "        [ -5.,  -8., -10.],\n",
            "        [-25., -23., -22.],\n",
            "        [ 26.,  25.,  23.]])\n",
            ">>> Bias of h1 [Layer 0] after forward:\n",
            "tensor([-15.5000, -16.5000,  -2.0000,  -6.0000,   6.0000])\n",
            "\n",
            ">>> Forward Output from relu1 [Layer 1]\n",
            "tensor([[  0.,   0.,   0.,   0., 299.],\n",
            "        [  0.,   0.,   0.,   0., 305.]], grad_fn=<ReluBackward0>)\n",
            "\n",
            ">>> Forward Output from h2 [Layer 2]\n",
            "tensor([[ -8387.5000, -14069.5000],\n",
            "        [ -8555.5000, -14351.5000]], grad_fn=<AddmmBackward0>)\n",
            ">>> Weights of h2 [Layer 2] after forward:\n",
            "tensor([[-11., -15., -21., -36., -28.],\n",
            "        [-22., -24., -25., -49., -47.]])\n",
            ">>> Bias of h2 [Layer 2] after forward:\n",
            "tensor([-15.5000, -16.5000])\n",
            "\n",
            ">>> Forward Output from relu2 [Layer 3]\n",
            "tensor([[0., 0.],\n",
            "        [0., 0.]], grad_fn=<ReluBackward0>)\n",
            "\n",
            ">>> Forward Output from h3 [Layer 4]\n",
            "tensor([[-15.5000, -16.5000,  -2.0000],\n",
            "        [-15.5000, -16.5000,  -2.0000]], grad_fn=<AddmmBackward0>)\n",
            ">>> Weights of h3 [Layer 4] after forward:\n",
            "tensor([[-1.3000e+01, -2.8000e+01],\n",
            "        [ 7.0000e+00,  1.4901e-08],\n",
            "        [ 1.0000e+00,  2.0000e+00]])\n",
            ">>> Bias of h3 [Layer 4] after forward:\n",
            "tensor([-15.5000, -16.5000,  -2.0000])\n",
            "\n",
            ">>> Forward Output from relu3 [Layer 5]\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
            "\n",
            ">>> Forward Output from h4 [Layer 6]\n",
            "tensor([[-15.5000, -16.5000],\n",
            "        [-15.5000, -16.5000]], grad_fn=<AddmmBackward0>)\n",
            ">>> Weights of h4 [Layer 6] after forward:\n",
            "tensor([[-42.,  -6., -72.],\n",
            "        [  0.,  -1.,   1.]])\n",
            ">>> Bias of h4 [Layer 6] after forward:\n",
            "tensor([-15.5000, -16.5000])\n",
            "\n",
            ">>> Forward Output from relu4 [Layer 7]\n",
            "tensor([[0., 0.],\n",
            "        [0., 0.]], grad_fn=<ReluBackward0>)\n",
            "\n",
            ">>> Forward Output from h5 [Layer 8]\n",
            "tensor([[-15.5000, -16.5000],\n",
            "        [-15.5000, -16.5000]], grad_fn=<AddmmBackward0>)\n",
            ">>> Weights of h5 [Layer 8] after forward:\n",
            "tensor([[  0.,   1.],\n",
            "        [-37., -67.]])\n",
            ">>> Bias of h5 [Layer 8] after forward:\n",
            "tensor([-15.5000, -16.5000])\n",
            "\n",
            ">>> Forward Output from relu5 [Layer 9]\n",
            "tensor([[0., 0.],\n",
            "        [0., 0.]], grad_fn=<ReluBackward0>)\n",
            "\n",
            ">>> Forward Output from h6 [Layer 10]\n",
            "tensor([[-15.5000, -16.5000],\n",
            "        [-15.5000, -16.5000]], grad_fn=<AddmmBackward0>)\n",
            ">>> Weights of h6 [Layer 10] after forward:\n",
            "tensor([[ 34.5000,  18.0000],\n",
            "        [-32.5000, -18.0000]])\n",
            ">>> Bias of h6 [Layer 10] after forward:\n",
            "tensor([-15.5000, -16.5000])\n",
            "\n",
            ">>> Forward Output from relu6 [Layer 11]\n",
            "tensor([[0., 0.],\n",
            "        [0., 0.]], grad_fn=<ReluBackward0>)\n",
            "\n",
            ">>> Forward Output from h7 [Layer 12]\n",
            "tensor([[2.5000],\n",
            "        [2.5000]], grad_fn=<AddmmBackward0>)\n",
            ">>> Weights of h7 [Layer 12] after forward:\n",
            "tensor([[15.5000, 51.5000]])\n",
            ">>> Bias of h7 [Layer 12] after forward:\n",
            "tensor([2.5000])\n",
            "Loss: 1.25\n",
            "**********\n",
            "2 2\n",
            "grad_output from loss: tensor(1.)\n",
            "grad_input from loss: tensor([[1.5000],\n",
            "        [0.5000]])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Track current epoch externally\n",
        "current_epoch = {'epoch': 0}  # Use dict to allow mutation from inner scope\n",
        "\n",
        "# Define the custom network with named ReLU layers\n",
        "class MyNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyNetwork, self).__init__()\n",
        "        self.h1 = nn.Linear(3, 5)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.h2 = nn.Linear(5, 2)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.h3 = nn.Linear(2, 3)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.h4 = nn.Linear(3, 2)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.h5 = nn.Linear(2, 2)\n",
        "        self.relu5 = nn.ReLU()\n",
        "        self.h6 = nn.Linear(2, 2)\n",
        "        self.relu6 = nn.ReLU()\n",
        "        self.h7 = nn.Linear(2, 1)\n",
        "\n",
        "        self.h1.weight.data = torch.tensor([[0, 0, 1],[0, 1, 0],[1, 0, 0],[1, 1, 0],[0, 1, 1]], dtype=torch.float32)\n",
        "        self.h2.weight.data = torch.tensor([[1, 1, -1, 0, 0],[0, 0, 1, 1, -1]], dtype=torch.float32)\n",
        "        self.h3.weight.data = torch.tensor([[1, 1],[1, -1],[1, 2]], dtype=torch.float32)\n",
        "        self.h4.weight.data = torch.tensor([[1, -1, 0],[0, -1, 1]], dtype=torch.float32)\n",
        "        self.h5.weight.data = torch.tensor([[0, 1],[1, 0]], dtype=torch.float32)\n",
        "        self.h6.weight.data = torch.tensor([[1, -1],[1, 1]], dtype=torch.float32)\n",
        "        self.h7.weight.data = torch.tensor([[1, -1]], dtype=torch.float32)\n",
        "\n",
        "        B = torch.tensor([0, 0, 0, 0, 0], dtype=torch.float32)\n",
        "        self.h1.bias.data = B\n",
        "        self.h2.bias.data = B[:2]\n",
        "        self.h3.bias.data = B[:3]\n",
        "        self.h4.bias.data = B[:2]\n",
        "        self.h5.bias.data = B[:2]\n",
        "        self.h6.bias.data = B[:2]\n",
        "        self.h7.bias.data = torch.tensor([0.], dtype=torch.float32)\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = self.relu1(self.h1(input))\n",
        "        out = self.relu2(self.h2(out))\n",
        "        out = self.relu3(self.h3(out))\n",
        "        out = self.relu4(self.h4(out))\n",
        "        out = self.relu5(self.h5(out))\n",
        "        out = self.relu6(self.h6(out))\n",
        "        out = self.h7(out)\n",
        "        return out\n",
        "\n",
        "# Custom MSE loss using autograd\n",
        "class CustomMSELossFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, target):\n",
        "        ctx.save_for_backward(input, target)\n",
        "        return ((input - target) ** 2).mean()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        print(\"*\"*10)\n",
        "        input, target = ctx.saved_tensors\n",
        "        print(input.numel(),input.size(0))\n",
        "        batch_size = input.size(0)\n",
        "        grad_input = 2.0 * (input - target) / batch_size #input.numel() --> Returns the total number of elements in the tensor, across all dimensions.\n",
        "\n",
        "        print(\"grad_output from loss:\", grad_output)\n",
        "        print(\"grad_input from loss:\", grad_input * grad_output)\n",
        "        return grad_input * grad_output, None\n",
        "\n",
        "# Register hooks\n",
        "def register_hooks(model, current_epoch):\n",
        "    hooks = []\n",
        "\n",
        "    def make_forward_hook(name, module):\n",
        "        def forward_hook(module, input, output):\n",
        "            if current_epoch['epoch'] == 1:  # Only print during epoch 2\n",
        "                print(f\"\\n>>> Forward Output from {name}\")\n",
        "                print(output)\n",
        "                if isinstance(module, nn.Linear):\n",
        "                    print(f\">>> Weights of {name} after forward:\")\n",
        "                    print(module.weight.data)\n",
        "                    if module.bias is not None:\n",
        "                        print(f\">>> Bias of {name} after forward:\")\n",
        "                        print(module.bias.data)\n",
        "        return forward_hook\n",
        "\n",
        "    layer_index = 0\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, (nn.Linear, nn.ReLU)):\n",
        "            hooks.append(module.register_forward_hook(make_forward_hook(f\"{name} [Layer {layer_index}]\", module)))\n",
        "            layer_index += 1\n",
        "\n",
        "    return hooks\n",
        "\n",
        "\n",
        "# Input and target\n",
        "x = torch.tensor([[3, 4, 5], [5, 4, 3]], dtype=torch.float32, requires_grad=True)\n",
        "target = torch.tensor([[1.], [2.]])\n",
        "\n",
        "# Initialize model\n",
        "model = MyNetwork()\n",
        "model.train()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# Register hooks\n",
        "hooks = register_hooks(model, current_epoch)\n",
        "\n",
        "# Train for 2 epochs\n",
        "for epoch in range(2):\n",
        "    current_epoch['epoch'] = epoch\n",
        "    print(f\"\\n========== EPOCH {epoch + 1} ==========\")\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output = model(x)\n",
        "    loss = CustomMSELossFunction.apply(output, target)\n",
        "    print(\"Loss:\", loss.item())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Clean up\n",
        "for h in hooks:\n",
        "    h.remove()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
